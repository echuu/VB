
Description of: model, setup, hyperparameters, variational parameters, algorithm

	Contents:
		1. Regression Models
		2. Hyperparameters
		3. CAVI
		4. Variational Approximation
		5. Averaging Over Hyperparameter Settings
		6. Prior on Proportion of Variance


    1. REGRESSION MODELS:

       Two types of outcomes Y are modeled: (1) a continuous outcome, which
       is also referred to as a "quantitative trait" in the genetics
       literature; or (2) a binary outcome with possible values 0 and 1. Y is
       modeled as a continuous outcome by setting family = 'gaussian'. In this
       case, Y is i.i.d. normal with mean u0 + Z*u + X*b and variance sigma,
       in which u and b are vectors of regresion coefficients, and u0 is the
       intercept. In the second case, we use logistic regression to model Y,
       in which the probability that Y = 1 is given by

           Pr(Y = 1) = sigmoid(u0 + Z*u + X*b).
       
       See 'help sigmoid' for a description of the sigmoid function. Note
       that the regression always includes an intercept term (u0).
       For both regression models, the fitting procedure consists of an inner
       loop and an outer loop. The outer loop iterates over each of the
       hyperparameter settings. The hyperparameters sa, sigma and logodds are
       specified by three arrays with the same number of elements, in which
       options.sa(i), options.sigma(i) and options.logodds(i) specify the ith
       hyperparameter setting. (The exception to this is when logodds is a
       matrix.) Note that sigma is only used for the linear regression model,
       and will generate an error if family = 'binomial'.


    2. HYPERPARAMETERS:

       Hyperparameter sa is the prior variance of regression coefficients for
       variables that are included in the model. This prior variance is always
       scaled by sigma (for logistic regression, we take sigma = 1).  Scaling
       the variance of the coefficients in this way is necessary to ensure
       that this prior is invariant to measurement scale (e.g., switching from
       grams to kilograms). Hyperparameter logodds is the prior log-odds that
       a variable is included in the regression model; it is defined as
       logodds = log10(q/(1-q)), where q is the prior probability that a
       variable is included in the regression model. Note that we use the
       base-10 logarithm instead of the natural logarithm because it is
       usually more natural to specify prior log-odds settings in this way.
       The prior log-odds may also be specified separately for each variable,
       which is useful if there is prior information about which variables are
       most relevant to the outcome Y. This is accomplished by setting
       options.logodds to a p x ns matrix, where p is the number of
       variables, and ns is the number of hyperparameter settings. In this
       case, fit.prior_same = false.

    
    3. CO-ORDINATE ASCENT OPTIMIZATION:

       Given a setting of the hyperparameters, options.sa(i), options.sigma(i)
       and options.logodds(:,i), the inner loop cycles through coordinate
       ascent updates to tighten the lower bound on the marginal likelihood,

           Pr(Y | X, sigma, sa, logodds).
       
       The inner loop coordinate ascent updates terminate when either (1) the
       maximum number of inner loop iterations is reached, as specified by
       options.maxiter, or (2) the maximum difference between the estimated
       posterior inclusion probabilities (see below) is less than options.tol.
       To provide a more accurate variational approximation of the posterior
       distribution, by default the fitting procedure has two stages:

       In the first stage, the entire fitting procedure is run to 
       completion, and the variational parameters (alpha, mu, s, eta) 
       corresponding to the maximum lower bound are then used to initialize the coordinate ascent updates in a second stage. Although this has the effect 
       of doubling the computation time (in the worst case), the final posterior 
       estimates tend to be more accurate with this two-stage fitting procedure. 
       The initial stage will be automatically skipped if initial estimates 
       of the variational parameters are provided in options.alpha, options.mu 
       and/or options.s, unless options.initialize_params = true.
       
       It is possible to optimize hyperparameter sa (and sigma for family =
       'gaussian') as part of the inner loop fitting procedure. Parameters sa
       and sigma will automatically be fitted to the data, separately for each
       hyperparameter setting, when options.sa and options.sigma are not
       specified. Alternatively, this can be achieved by setting
       options.update_sa = true and options.update_sigma = true, in which case
       options.sa and options.sigma are treated as initial estimates of these
       parameters if they are provided. These parameters are fitted by
       computing approximate maximum-likelihood (ML) estimates. 

       Optionally, an approximate maximum a posteriori (MAP) estimate of sa 
       is computed by setting options.sa0 and options.n0 to positive scalars; 
       these two numbers specify the scale parameter and number of degrees of 
       freedom for a scaled inverse chi-square prior on sa. Large settings of n0
       provide greater stability of the parameter estimates for cases when the
       model is "sparse"; that is, when few variables are included in the
       model.

       Note it is not possible to fit the logodds parameter; if
       options.logodds is not provided, then it is set to the default value
       when options.sa and options.sigma are scalars, and otherwise an error
       is generated.


    4. VARIATIONAL APPROXIMATION:

       Outputs fit.alpha, fit.mu and fit.s specify the approximate posterior
       distribution of the regression coefficients. Each of these outputs is a
       p x ns matrix. For the ith hyperparameter setting, fit.alpha(:,i) is
       the variational estimate of the posterior inclusion probability (PIP)
       for each variable; fit.mu(:,i) is the variational estimate of the
       posterior mean coefficient given that it is included in the model; and
       fit.s(:,i) is the estimated posterior variance of the coefficient given
       that it is included in the model. These are also the quantities that
       are optimized as part of the inner loop coordinate ascent updates. From
       these quantities, we also provide fit.pve for family = 'gaussian'. This
       output provides, for each hyperparameter setting, the mean estimate of
       the proportion of variance in Y explained by each of the variables
       conditioned on being included in the model.

       An additional variational parameter, denoted by 'eta', is needed for
       fast computation with the logistic regression model (family =
       'binomial'). The fitted value of eta is returned as an n x ns matrix
       fit.eta. If a good estimate of eta is already available (e.g., in a
       previous call to varbvs on the same data), provide this estimate in
       options.eta, in which case eta is not fitted to the data during the
       inner loop coordinate ascent updates (to override this behaviour, set
       options.optimize_eta = true, in which case options.eta is treated as an
       initial estimate).

       The variational estimates should be interpreted carefully, especially
       when variables are strongly correlated. For example, consider the
       simple scenario in which 2 candidate variables are closely correlated,
       and at least one of them explains the outcome with probability close to
       1. Under the correct posterior distribution, we would expect that each
       variable is included with probability ~0.5. However, the variational
       approximation, due to the conditional independence assumption, will
       typically get this wrong, and concentrate most of the posterior weight
       on one variable (the actual variable that is chosen will depend on the
       starting conditions of the optimization). Although the individual PIPs
       are incorrect, a statistic summarizing the variable selection for both
       correlated variables (e.g., the total number of variables included in
       the model) should be reasonably accurate.

       More generally, if variables can be reasonably grouped together based
       on their correlations, we recommend interpreting the variable selection
       results at a group level. For example, in genome-wide association
       studies (see the vignettes) ,a SNP with a high PIP indicates that this
       SNP is probably associated with the trait, and one or more nearby SNPs
       within a chromosomal region, or ``locus,'' may be associated as
       well. Therefore, we interpreted the GWAS variable selection results at
       the level of loci, rather than at the level of individual SNPs.
       Also note that special care is required for interpreting the results of
       the variational approximation with the logistic regression model. In
       particular, interpretation of the individual estimates of the
       regression coefficients (e.g., the posterior mean estimates fit.mu) is
       not straightforward due to the additional approximation introduced on
       the individual nonlinear factors in the likelihood. As a general
       guideline, only the relative magnitudes of the coefficients are
       meaningful.


    5. AVERAGING OVER HYPERPARAMETER SETTINGS:

       Output fit.logw is an array with ns elements, in which fit.logw(i) is
       the variational lower bound on the marginal log-likelihood for the ith
       setting of the hyperparameters. In many settings, it is good practice
       to account for uncertainty in the hyperparameters when reporting final
       posterior quantities. Provided that (1) the hyperparameter settings
       options.sigma, options.sa and options.logodds adequately represent the
       space of possible hyperparameter settings with high posterior mass, (2)
       the hyperparameter settings are drawn from the same distribution as the
       prior, and (3) the fully-factorized variational approximation closely
       approximates the true posterior distribution, then final posterior
       quantities can be calculated by using fit.logw as (unnormalized)
       log-marginal probabilities. Even when conditions (1), (2) and/or (3)
       are not satisfied, this can approach can still often yield reasonable
       estimates of averaged posterior quantities. For example, do the
       following to compute the posterior mean estimate of sa:

           mean_sa = dot(fit.sa(:),fit.w(:));

       This is precisely how final posterior quantities are reported by
       varbvsprint (type 'help varbvsprint' for more details). To account for
       discrepancies between the prior on (sigma,sa,logodds) and the sampling
       density used to draw candidate settings of the hyperparameters, adjust
       the log-marginal probabilities weights by setting fit.logw = fit.logw +
       logp/logq, where logp is the log-density of the prior distribution, and
       logq is the log-density of the sampling distribution. 
       
       (This is importance sampling; see, for example, R. M. Neal, 
       "Annealed importance sampling", Statistics and Computing, 2001.)


    6. PRIOR ON PROPORTION OF VARIANCE EXPLAINED:

       Specifying the prior variance of the regression coefficients (sa) can
       be difficult, which is why we have included the option of fitting this
       hyperparameter to the data (see input argument update_sa
       above). However, in many settings, especially when a small number of
       variables are included in the regression model, it is preferrable to
       average over candidate settings of sa instead of fitting sa to the
       data. To choose a set of candidate settings for sa, we have advocated
       for setting sa indirectly through a prior estimate of the proportion of
       variance in the outcome explained by the variables (abbreviated as
       PVE), since it is often more natural to specify the PVE rather than the
       prior variance (see references below). This is technically only
       suitable or the linear regression model (family = 'gaussian'), but
       could potentially be used for the linear regression model in an
       approximate way.
       For example, one could approximate a uniform prior on the PVE by
       drawing the PVE uniformly between 0 and 1, additionally specifying
       candidate settings for the prior log-odds, then computing the prior
       variance (sa) as follows:

         X  = X - repmat(mean(X),size(X,1),1);
         sx = sum(var1(X));
         sa = PVE./(1-PVE)./(sigmoid(log(10)*logodds)*sx)}
    
       Note that this calculation will yield sa = 0 when PVE = 0, and sa = Inf
       when PVE = 1. The first line sets the mean of each column of X to 0,
       which is needed to ensure that var1 correctly computes the sample
       variance of each variable.
    
       Also, bear in mind that if there are additional covariates (Z) included
       in the linear regression model that explain variance in Y, then it will
       usually make more sense to first remove the linear effects of these
       covariates before performing this calculation. The PVE would then
       represent the prior proportion of variance in the residuals of Y that
       are explained by the candidate variables. For an example of how to do
       this, see varbvs.m, under "preprocessing steps". Alternatively, one
       could include the matrix Z in the calculation above, taking care to
       ensure that the covariates are included in the model with
       probability 1.